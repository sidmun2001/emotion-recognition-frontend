{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUSMXkyxed38"
   },
   "source": [
    "# **1. Mount to Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3VNgikPdzp5",
    "outputId": "86dbf2cf-b19b-4046-c46f-0093059b79ab"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jw94wK1XfFPf"
   },
   "source": [
    "# **2. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4m4fdylFfFuq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsIuSl8tfKuu"
   },
   "source": [
    "# ***Imports for Performance Metrics/Visualization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R3IDexqxfSIZ"
   },
   "outputs": [],
   "source": [
    "# Imports for Performance Metrics/Visualization\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    f1_score, precision_recall_curve, PrecisionRecallDisplay,\n",
    "    accuracy_score, roc_curve, RocCurveDisplay\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkitlbZQfVtv"
   },
   "source": [
    "# **3. Setup Directories to Training, Validation, and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5Bpmo1SfWMX",
    "outputId": "aff50be5-64d2-4195-a974-885ddcb1049f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Directory: G:/.shortcut-targets-by-id/1PfaEKQoN9ziSq5ctd1Kwcm8wcEnWPruy/CAPSTONE 2024 2025/COE70B - Sem 2/Phase 1/Prayash/Emotions\\train\n",
      "Validation Directory: G:/.shortcut-targets-by-id/1PfaEKQoN9ziSq5ctd1Kwcm8wcEnWPruy/CAPSTONE 2024 2025/COE70B - Sem 2/Phase 1/Prayash/Emotions\\validation\n",
      "Testing Directory: G:/.shortcut-targets-by-id/1PfaEKQoN9ziSq5ctd1Kwcm8wcEnWPruy/CAPSTONE 2024 2025/COE70B - Sem 2/Phase 1/Prayash/Emotions\\test\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'G:/.shortcut-targets-by-id/1PfaEKQoN9ziSq5ctd1Kwcm8wcEnWPruy/CAPSTONE 2024 2025/COE70B - Sem 2/Phase 1/Prayash/Emotions'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "print(\"Training Directory:\", train_dir)\n",
    "print(\"Validation Directory:\", val_dir)\n",
    "print(\"Testing Directory:\", test_dir)\n",
    "\n",
    "# Directory to save augmented images\n",
    "augmented_train_dir = os.path.join(base_dir, 'augmented_train')\n",
    "\n",
    "# Create the augmented directory if it doesn't exist\n",
    "if not os.path.exists(augmented_train_dir):\n",
    "    os.makedirs(augmented_train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qex4y3pjcrD"
   },
   "source": [
    "# **4. Data Augmentation and Saving Augmented Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWY8S0arjbdX",
    "outputId": "59e91b32-c1c4-4caa-ec2b-9ac138cc2306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented images already exist. Skipping augmentation.\n"
     ]
    }
   ],
   "source": [
    "# Define the augmentation pipeline\n",
    "augmentation_pipeline = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values\n",
    "    rotation_range=20,  # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.2,  # Randomly shift images horizontally by up to 20%\n",
    "    height_shift_range=0.2,  # Randomly shift images vertically by up to 20%\n",
    "    shear_range=0.2,  # Apply shearing transformations\n",
    "    zoom_range=0.2,  # Randomly zoom in/out by up to 20%\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    brightness_range=[0.8, 1.2],  # Adjust brightness\n",
    "    fill_mode='nearest'  # Fill in missing pixels after transformations\n",
    ")\n",
    "\n",
    "# Number of augmented copies to create per image\n",
    "num_augmented_copies = 5\n",
    "\n",
    "# Check if the augmented_train directory already exists and contains images\n",
    "if os.path.exists(augmented_train_dir) and any(os.listdir(augmented_train_dir)):\n",
    "    print(\"Augmented images already exist. Skipping augmentation.\")\n",
    "else:\n",
    "    # Loop through each class directory\n",
    "    for class_name in os.listdir(train_dir):\n",
    "        class_dir = os.path.join(train_dir, class_name)\n",
    "        augmented_class_dir = os.path.join(augmented_train_dir, class_name)\n",
    "\n",
    "        # Create the augmented class directory if it doesn't exist\n",
    "        if not os.path.exists(augmented_class_dir):\n",
    "            os.makedirs(augmented_class_dir)\n",
    "\n",
    "        # Loop through each image in the class directory\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_name)\n",
    "\n",
    "            # Skip non-image files (e.g., hidden files like .DS_Store)\n",
    "            if not image_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                print(f\"Skipping non-image file: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Load the image\n",
    "                image = load_img(image_path)\n",
    "                image_array = img_to_array(image)  # Convert to numpy array\n",
    "                image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "\n",
    "                # Generate augmented images\n",
    "                augmented_images = augmentation_pipeline.flow(\n",
    "                    image_array,\n",
    "                    batch_size=1,\n",
    "                    save_to_dir=augmented_class_dir,  # Save to the emotion-specific folder\n",
    "                    save_prefix=f'aug_{class_name}',\n",
    "                    save_format='jpg'\n",
    "                )\n",
    "\n",
    "                # Save the augmented images\n",
    "                for i in range(num_augmented_copies):\n",
    "                    next(augmented_images)\n",
    "\n",
    "            except UnidentifiedImageError:\n",
    "                print(f\"Cannot identify image file: {image_path}. Skipping.\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {image_path}: {e}. Skipping.\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PuSfNmmjg8v"
   },
   "source": [
    "# **5. Create Data Generators for Augmented Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I-RkJ0i6jhPj",
    "outputId": "9aa69c57-c2c8-408d-eb5e-0b1147df1c61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2616 images belonging to 8 classes.\n",
      "Found 42 images belonging to 8 classes.\n",
      "Found 0 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation for Training Data (already applied, so no need for additional augmentation)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Validation and Test Data Generators (No Augmentation)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create Generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    augmented_train_dir,  # Use the augmented dataset\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwvJzAe_fuom"
   },
   "source": [
    "# **6. Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUKKGZBCjvrg",
    "outputId": "95e76b01-a231-4c2f-c50b-add2bda791a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 544ms/step - accuracy: 0.1470 - loss: 3.3522\n",
      "Epoch 2/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 601ms/step - accuracy: 0.1495 - loss: 3.1661\n",
      "Epoch 3/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 582ms/step - accuracy: 0.1943 - loss: 2.8494\n",
      "Epoch 4/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 556ms/step - accuracy: 0.1960 - loss: 2.8136\n",
      "Epoch 5/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 552ms/step - accuracy: 0.2027 - loss: 2.7365\n",
      "Epoch 6/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 538ms/step - accuracy: 0.2281 - loss: 2.6095\n",
      "Epoch 7/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 548ms/step - accuracy: 0.2363 - loss: 2.5974\n",
      "Epoch 8/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 611ms/step - accuracy: 0.2477 - loss: 2.4577\n",
      "Epoch 9/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 586ms/step - accuracy: 0.2588 - loss: 2.4153\n",
      "Epoch 10/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 602ms/step - accuracy: 0.2691 - loss: 2.3317\n",
      "Epoch 11/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 623ms/step - accuracy: 0.2855 - loss: 2.3411\n",
      "Epoch 12/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 556ms/step - accuracy: 0.2622 - loss: 2.4151\n",
      "Epoch 13/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 551ms/step - accuracy: 0.2788 - loss: 2.2720\n",
      "Epoch 14/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 598ms/step - accuracy: 0.3169 - loss: 2.1444\n",
      "Epoch 15/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 602ms/step - accuracy: 0.3117 - loss: 2.1801\n",
      "Epoch 16/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 565ms/step - accuracy: 0.3113 - loss: 2.1349\n",
      "Epoch 17/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 569ms/step - accuracy: 0.3181 - loss: 2.0942\n",
      "Epoch 18/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 594ms/step - accuracy: 0.3267 - loss: 2.0686\n",
      "Epoch 19/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 569ms/step - accuracy: 0.3417 - loss: 2.0100\n",
      "Epoch 20/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 563ms/step - accuracy: 0.3418 - loss: 1.9823\n",
      "Epoch 21/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 576ms/step - accuracy: 0.3497 - loss: 1.9978\n",
      "Epoch 22/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 566ms/step - accuracy: 0.3390 - loss: 1.9881\n",
      "Epoch 23/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 563ms/step - accuracy: 0.3580 - loss: 1.9230\n",
      "Epoch 24/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 560ms/step - accuracy: 0.3746 - loss: 1.8373\n",
      "Epoch 25/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 567ms/step - accuracy: 0.3653 - loss: 1.9001\n",
      "Epoch 26/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 567ms/step - accuracy: 0.3786 - loss: 1.8231\n",
      "Epoch 27/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 604ms/step - accuracy: 0.3788 - loss: 1.7998\n",
      "Epoch 28/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 551ms/step - accuracy: 0.3881 - loss: 1.7483\n",
      "Epoch 29/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 518ms/step - accuracy: 0.3857 - loss: 1.8153\n",
      "Epoch 30/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 532ms/step - accuracy: 0.4154 - loss: 1.7471\n",
      "Epoch 31/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 530ms/step - accuracy: 0.3820 - loss: 1.7497\n",
      "Epoch 32/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 521ms/step - accuracy: 0.3935 - loss: 1.7422\n",
      "Epoch 33/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 531ms/step - accuracy: 0.4197 - loss: 1.6700\n",
      "Epoch 34/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 545ms/step - accuracy: 0.4213 - loss: 1.6667\n",
      "Epoch 35/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 580ms/step - accuracy: 0.4282 - loss: 1.6746\n",
      "Epoch 36/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 561ms/step - accuracy: 0.4192 - loss: 1.6735\n",
      "Epoch 37/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 542ms/step - accuracy: 0.4185 - loss: 1.6366\n",
      "Epoch 38/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 541ms/step - accuracy: 0.4176 - loss: 1.6716\n",
      "Epoch 39/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 564ms/step - accuracy: 0.4344 - loss: 1.6195\n",
      "Epoch 40/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 559ms/step - accuracy: 0.4350 - loss: 1.5660\n",
      "Epoch 41/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 585ms/step - accuracy: 0.4575 - loss: 1.5508\n",
      "Epoch 42/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 568ms/step - accuracy: 0.4198 - loss: 1.6102\n",
      "Epoch 43/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 587ms/step - accuracy: 0.4592 - loss: 1.5298\n",
      "Epoch 44/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 527ms/step - accuracy: 0.4419 - loss: 1.5849\n",
      "Epoch 45/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 542ms/step - accuracy: 0.4625 - loss: 1.5112\n",
      "Epoch 46/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 535ms/step - accuracy: 0.4756 - loss: 1.4941\n",
      "Epoch 47/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 564ms/step - accuracy: 0.4780 - loss: 1.4673\n",
      "Epoch 48/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 731ms/step - accuracy: 0.4380 - loss: 1.5844\n",
      "Epoch 49/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 559ms/step - accuracy: 0.4588 - loss: 1.4946\n",
      "Epoch 50/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 562ms/step - accuracy: 0.4696 - loss: 1.4901\n",
      "Epoch 51/100\n",
      "\u001b[1m58/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 575ms/step - accuracy: 0.4590 - loss: 1.4865"
     ]
    }
   ],
   "source": [
    "def create_transfer_learning_model():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)  # Increase neurons\n",
    "    x = BatchNormalization()(x)  # Add Batch Normalization\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)  # Additional dense layer\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    predictions = Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the Model\n",
    "transfer_learning_model = create_transfer_learning_model()\n",
    "\n",
    "# Train the Model (without validation set)\n",
    "transfer_learning_history = transfer_learning_model.fit(\n",
    "    train_generator,\n",
    "    epochs=100  # Train for a fixed number of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LifFodryjxoX"
   },
   "source": [
    "# **7. Performance Metrics and Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZjSi0-aBjyDd",
    "outputId": "b1c09fae-7ce3-4994-98e2-a538cf8104d7"
   },
   "outputs": [],
   "source": [
    "# Part 1: Accuracy, Loss\n",
    "train_accuracy = transfer_learning_history.history['accuracy']\n",
    "#val_accuracy = transfer_learning_history.history['val_accuracy']\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy[-1] * 100:.2f}%\")\n",
    "#print(f\"Validation Accuracy: {val_accuracy[-1] * 100:.2f}%\")\n",
    "\n",
    "# Evaluate transfer learning model on the validation set\n",
    "transfer_learning_scores = transfer_learning_model.evaluate(val_generator)\n",
    "print(f\"Test Loss: {transfer_learning_scores[0]:.4f}, Test Accuracy: {transfer_learning_scores[1] * 100:.2f}%\")\n",
    "\n",
    "# Part 2: Learning and Validation Curves\n",
    "# Accuracy vs. Epochs\n",
    "plt.figure()\n",
    "plt.plot(transfer_learning_history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "#plt.plot(transfer_learning_history.history['val_accuracy'], label='Validation Accuracy', color='green')\n",
    "plt.title(\"Accuracy vs. Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Loss vs. Epochs\n",
    "plt.plot(transfer_learning_history.history['loss'], label='Training Loss')\n",
    "#plt.plot(transfer_learning_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"Loss vs. Epochs\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Part 3: Confusion Matrix\n",
    "# Get model predictions\n",
    "y_prob = transfer_learning_model.predict(val_generator)  # Probabilities for each class\n",
    "y_pred = np.argmax(y_prob, axis=1)  # Predicted class indices\n",
    "y_true = val_generator.labels  # True labels\n",
    "\n",
    "emotion_indices = val_generator.class_indices  # Maps class names to indices\n",
    "emotion_names = list(emotion_indices.keys())  # List of class names\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=emotion_names)\n",
    "plt.figure(figsize=(10, 6))\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Part 4: Precision-Recall Curve\n",
    "y_true_binarized = label_binarize(y_true, classes=range(len(emotion_names)))\n",
    "for i, emotion in enumerate(emotion_names):\n",
    "    precision, recall, _ = precision_recall_curve(y_true_binarized[:, i], y_prob[:, i])\n",
    "    plt.plot(recall, precision, marker='.', label=emotion)\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Part 5: ROC Curve\n",
    "for i, emotion in enumerate(emotion_names):\n",
    "    fpr, tpr, _ = roc_curve(y_true_binarized[:, i], y_prob[:, i])\n",
    "    plt.plot(fpr, tpr, marker='.', label=emotion)\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Part 6: F1 Score (weighted), Accuracy\n",
    "# F1 Score (weighted)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 Score (weighted): {f1:.2f}\")\n",
    "\n",
    "# Accuracy Metric\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"emotion_model.h5\")  # Saves the model in HDF5 format\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
